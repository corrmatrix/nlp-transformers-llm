
## Purpose
Assemble a valuable collection of latest information and learning resources on NLP, Transformers and Large Language Model.

## Last News


- Meta's Code Llama : https://ai.meta.com/blog/code-llama-large-language-model-coding/ - 24/08/2023

- GPT-3.5 Turbo fine-tuning and API updates : https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates - 22/08/2023

- Bringing the world closer together with a foundational multimodal model for speech translation : https://ai.meta.com/blog/seamless-m4t - 22/08/2023

- AI2 Dolma: 3 Trillion Token Open Corpus for Language Model Pretraining : https://blog.allenai.org/dolma-3-trillion-tokens-open-llm-corpus-9a0ff4b8da64 - 19/08/2023

- Can You Run LLaMA and Llama-2 Ai Model Locally? : https://www.hardware-corner.net/run-llama-ai-locally/ (see also https://www.hardware-corner.net/guides/computer-to-run-llama-ai-model/) - 02/08/2023

- Stability Diffusion - SD-XL 1.0-base Model Card : https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0 - 28/07/2023

- Stack overflow search - OverflowAI : https://twitter.com/StackOverflow/status/1684530704850243584 - 27/07/2023

- Meta's Open Source Llama Upsets the AI Horse Race: https://www.wired.com/story/metas-open-source-llama-upsets-the-ai-horse-race - 26/07/2023

- Patterns for Building LLM-based Systems & Products : https://eugeneyan.com/writing/llm-patterns/ - 23/07/2023

- Bringing General AI to search experience : https://cloud.google.com/blog/products/ai-machine-learning/enterprise-search-on-gen-app-builder - 21/07/2023

- Cognaize raises $18M to build a better LLM for the finance sector, one that keeps humans in the loop : https://techcrunch.com/2023/07/18/cognaize-raises-18m-to-build-a-better-llm-for-the-finance-sector-one-that-keeps-humans-in-the-loop/

- Meta and Microsoft Introduce the Next Generation of Llama : https://about.fb.com/news/2023/07/llama-2 - 18/07/2023

- Bard's latest update: more features, languages and countries : https://blog.google/products/bard/google-bard-new-features-update-july-2023 - 13/07/2023

- Anthropic's 'friendly' AI chatbot, Claude, is now available for more people to try : https://www.theverge.com/2023/7/11/23790254/anthropic-claude-chatbot-ai-available-beta - Only available for USA and UK - 11/07/2023

- AI tools are designing entirely new proteins that could transform medicine : https://www.nature.com/articles/d41586-023-02227-y - 11/07/2023

- GPT-4 Architecture, datasets and costs leaked : https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked - Mixture of Experts (MoE) - 11/07/2023

- Meet LongLLaMA: A Large Language Model Capable of Handling Long Contexts of 256k Tokens : https://www.marktechpost.com/2023/07/10/meet-longllama-a-large-language-model-capable-of-handling-long-contexts-of-256k-tokens - https://github.com/cstankonrad/long_llama - 10/07/2023

- Claude vs ChatGPT for Data Science: A Comparative Analysis : https://www.datacamp.com/blog/claude-vs-chatgpt-data-science-comparison  - 30/06/2023

- Databricks Strikes $1.3 Billion Deal for Generative AI Startup MosaicML : https://www.wsj.com/articles/databricks-strikes-1-3-billion-deal-for-generative-ai-startup-mosaicml-fdcefc06 - 30/06/2023

- GPT-4's Secret Has Been Revealed : https://thealgorithmicbridge.substack.com/p/gpt-4s-secret-has-been-revealed -> https://twitter.com/swyx/status/1671272883379908608 (George Hotz) - GPT-4 - Mixture of smaller models - 24/06/2023

- AI is killing the old web, and the new web struggles to be born : https://www.theverge.com/2023/6/26/23773914/ai-large-language-models-data-scraping-generation-remaking-web - 27/06/2023

- AI Package Hallucinations : https://vulcan.io/blog/ai-hallucinations-package-risk - 06/06/2023

- Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models : https://huggingface.co/papers/2305.14705 -> https://arxiv.org/abs/2305.14705 - 24/05/2023

- Meta open-sources multisensory AI model that combines six types of data : https://www.theverge.com/2023/5/9/23716558/meta-imagebind-open-source-multisensory-modal-ai-model-research - 10/05/2023

- Google "We Have No Moat, And Neither Does OpenAI" : https://www.semianalysis.com/p/google-we-have-no-moat-and-neither - 04/05/2023

- Model that uses machine learning methods and patient data at hospital arrival predicts strokes more accurately than current system : https://www.newswise.com/articles/machine-learning-model-predicts-strokes-more-accurately-than-current-system - 19/04/2023

- Adobe launches AI-powered text-based video editing : https://venturebeat.com/games/adobe-launches-ai-powered-text-based-video-editing - 13/04/2023

- The LLama Effect: How an Accidental Leak Sparked a Series of Impressive Open Source Alternatives to ChatGPT : https://thesequence.substack.com/p/the-llama-effect-how-an-accidental - 09/04/2023

- Anthropic's $5B, 4-year plan to take on OpenAI : https://techcrunch.com/2023/04/06/anthropics-5b-4-year-plan-to-take-on-openai - 07/04/2023

- The takeaways from Stanford's 386-page report on the state of AI : https://techcrunch.com/2023/04/04/the-takeaways-from-stanfords-386-page-report-on-the-state-of-ai - 05/04/2023

- Introducing BloombergGPT, Bloomberg’s 50-billion parameter large language model, purpose-built from scratch for finance : https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance - 30/03/2023

- GPT-4 has more than a trillion parameters: https://the-decoder.com/gpt-4-has-a-trillion-parameters - 25/03/2023

- Our latest health AI research updates : https://blog.google/technology/health/ai-llm-medpalm-research-thecheckup - 14/03/2023

- Introducing Claude : https://www.anthropic.com/index/introducing-claude - 14/03/2023

- Alpaca: A Strong, Replicable Instruction-Following Model : https://crfm.stanford.edu/2023/03/13/alpaca.html - 13/03/2023

- Meta's powerful AI language model has leaked online — what happens now?: https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse - 09/03/2023

- Welcome to State of AI Report 2022: https://www.stateof.ai/2022-report-launch.html - 11/10/2022


## Articles and research papers

- Introduction to Weight Quantization : https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c

- GPTQ: ACCURATE POST-TRAINING QUANTIZATION
FOR GENERATIVE PRE-TRAINED TRANSFORMERS : https://arxiv.org/pdf/2210.17323.pdf - 22/03/2023

- Context-faithful Prompting for Large Language Models : https://arxiv.org/abs/2303.11315 - 20/03/2023

- LLaMA: Open and Efficient Foundation Language Models : https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models - 24/02/2023

- Understanding Large Language Models : A Transformative Reading List :  https://sebastianraschka.com/blog/2023/llm-reading-list.html - 07/02/2023

- Openai Research : https://openai.com/research

- Large Language Models are Zero-Shot Reasoners : https://arxiv.org/abs/2205.11916 - 24/02/2022

- (GPT1) Improving Language Understanding
by Generative Pre-Training : https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf - see also https://openai.com/research/language-unsupervised and https://huggingface.co/docs/transformers/model_doc/openai-gpt - 2018

- (GPT-2) Language Models are Unsupervised Multitask Learners (2019): https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf - see also https://openai.com/research/better-language-models and https://github.com/openai/gpt-2 (source code) - 2019

- (GPT-3) Language models are few-shot learners : https://openai.com/research/language-models-are-few-shot-learners and https://arxiv.org/abs/2005.14165 - May 28, 2020

- Fast Transformer Decoding: One Write-Head is All You Need : https://arxiv.org/abs/1911.02150 - 06/11/2019

- How GPT3 Works - Visualizations and Animations - https://jalammar.github.io/how-gpt3-works-visualizations-animations/

- The Illustrated GPT-2 (Visualizing Transformer Language Models) : https://jalammar.github.io/illustrated-gpt2

- Generalized Language Models : https://lilianweng.github.io/posts/2019-01-31-lm - 31/01/2019

- Language Models are Unsupervised Multitask Learners : https://paperswithcode.com/paper/language-models-are-unsupervised-multitask -> https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf - 2019

- A Visual Guide to Using BERT for the First Time : https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time - 2018

- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding : https://arxiv.org/abs/1810.04805 - 11/10/2018

- Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing : https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html - 02/11/2018

- Attention? Attention! : https://lilianweng.github.io/posts/2018-06-24-attention - 24/06/2018

- The Annotated Transformer : http://nlp.seas.harvard.edu/2018/04/03/attention.html - 03/04/2018

- Attention Is All You Need : https://arxiv.org/abs/1706.03762 and https://arxiv.org/pdf/1706.03762.pdf - 12/06/2017

## Resources

### Github and softwares


- bitsandbytes : https://github.com/TimDettmers/bitsandbytes - a lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and quantization functions

- BIG-Bench Hard : https://github.com/suzgunmirac/BIG-Bench-Hard

- Dalai : https://github.com/cocktailpeanut/dalai - Run LLaMA and Alpaca on your computer

- GGML : https://github.com/ggerganov/ggml

- koboldcpp : https://github.com/LostRuins/koboldcpp/wiki

- langchain : https://github.com/langchain-ai/langchain

- LoRA: Low-Rank Adaptation of Large Language Models : https://github.com/microsoft/LoRA

- Llama : https://github.com/facebookresearch/llama

- llama.cpp : https://github.com/ggerganov/llama.cpp

- LMSYS - Fastchat: https://github.com/lm-sys/FastChat -> Vicuna : https://lmsys.org/blog/2023-03-30-vicuna - Cost of training Vicuna-13B is around $300 -> Vicuna Installation Guide : https://github.com/vicuna-tools/vicuna-installation-guide

- Med-Flamingo : https://github.com/snap-stanford/med-flamingo

- Parameter-Efficient Fine-Tuning (PEFT) methods : https://github.com/huggingface/peft - enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters

- QLoRA: Efficient Finetuning of Quantized LLMs : https://github.com/artidoro/qlora

- RedPajama-Data : https://github.com/togethercomputer/RedPajama-Data - An Open Source Recipe to Reproduce LLaMA training dataset

- Stable Diffusion web UI : https://github.com/AUTOMATIC1111/stable-diffusion-webui

- Web LLM : https://github.com/mlc-ai/web-llm

- WizardVicunaLM : https://github.com/melodysdreamj/WizardVicunaLM - Wizard's dataset + ChatGPT's conversation extension + Vicuna's tuning method

- Huggingface : https://huggingface.co

- Modular - mojo : https://www.modular.com/mojo

- OpenAI : https://openai.com

- Midjourney : https://www.midjourney.com

- Stable diffusion : https://stability.ai/stablediffusion




### Learning/Education


- Practical Deep Learning : https://course.fast.ai/Lessons/lesson1.html

- Introduction to Large Lange Models : https://docs.cohere.com/docs/introduction-to-large-language-models

- LLM University! : https://docs.cohere.com/docs/llmu

- LLM Bootcamp - Spring 2023 : https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/

- Machine learning mastery : https://machinelearningmastery.com/start-here/

- The transformer model, explained clearly : https://www.deriveit.org/notes/119

- Geometric Deep Learning : https://geometricdeeplearning.com/blogs/

- Attention is all you need; Attentional Neural Network Models : https://www.youtube.com/watch?v=rBCqOTEfxvg - Oct 2017

- How GPT3 Works - Easily Explained with Animations : https://www.youtube.com/watch?v=MQnJZuBGmSQ - Aug 2020 - Basic overview

- The Narrated Transformer Language Model : https://www.youtube.com/watch?v=-QH8fRhqFHM - Aug 2020 - Details on architecture

- Interfaces for Explaining Transformer Language Models: https://jalammar.github.io/explaining-transformers - 2020

- The little book of deep learning : https://fleuret.org/public/lbdl.pdf



### Discord and related github

- Dalai: https://discord.gg/WWfgrzzkCT and https://github.com/cocktailpeanut/dalai - Run LLaMA and Alpaca on your computer

- FastChat (LMSys) : https://discord.gg/HSWAKCrnFx and https://github.com/lm-sys/FastChat and https://chat.lmsys.org - FastChat is an open platform for training, serving, and evaluating large language model based chatbots

- Text generation web UI: https://discord.gg/jwZCF2dPQN and https://github.com/oobabooga/text-generation-webui

- Tom Jobbins (TheBloke) : https://discord.gg/theblokeai and https://github.com/TheBloke

