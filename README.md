## Purpose
Assemble a valuable collection of latest information and learning resources on NLP, Transformers and Large Language Model.

## Last News

- Model that uses machine learning methods and patient data at hospital arrival predicts strokes more accurately than current system : https://www.newswise.com/articles/machine-learning-model-predicts-strokes-more-accurately-than-current-system - 19/04/2023

- Adobe launches AI-powered text-based video editing : https://venturebeat.com/games/adobe-launches-ai-powered-text-based-video-editing - 13/04/2023

- The LLama Effect: How an Accidental Leak Sparked a Series of Impressive Open Source Alternatives to ChatGPT : https://thesequence.substack.com/p/the-llama-effect-how-an-accidental - 09/04/2023

- Anthropic's $5B, 4-year plan to take on OpenAI : https://techcrunch.com/2023/04/06/anthropics-5b-4-year-plan-to-take-on-openai - 07/04/2023

- The takeaways from Stanford's 386-page report on the state of AI : https://techcrunch.com/2023/04/04/the-takeaways-from-stanfords-386-page-report-on-the-state-of-ai - 05/04/2023

- Introducing BloombergGPT, Bloomberg’s 50-billion parameter large language model, purpose-built from scratch for finance : https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance - 30/03/2023

- GPT-4 has more than a trillion parameters: https://the-decoder.com/gpt-4-has-a-trillion-parameters - 25/03/2023

- Our latest health AI research updates : https://blog.google/technology/health/ai-llm-medpalm-research-thecheckup - 14/03/2023

- Introducing Claude : https://www.anthropic.com/index/introducing-claude - 14/03/2023

- Alpaca: A Strong, Replicable Instruction-Following Model : https://crfm.stanford.edu/2023/03/13/alpaca.html - 13/03/2023

- Meta's powerful AI language model has leaked online — what happens now?: https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse - 09/03/2023

- Welcome to State of AI Report 2022: https://www.stateof.ai/2022-report-launch.html - 11/10/2022

## Articles and research papers

- GPTQ: ACCURATE POST-TRAINING QUANTIZATION
FOR GENERATIVE PRE-TRAINED TRANSFORMERS : https://arxiv.org/pdf/2210.17323.pdf - 22/03/2023

- Understanding Large Language Models : A Transformative Reading List :  https://sebastianraschka.com/blog/2023/llm-reading-list.html - **Sebastian Raschka** - Feb 7, 2023

- Openai Research : https://openai.com/research

- (GPT1) Improving Language Understanding
by Generative Pre-Training : https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf - see also https://openai.com/research/language-unsupervised and https://huggingface.co/docs/transformers/model_doc/openai-gpt - **Radford et. al.** - 2018

- (GPT-2) Language Models are Unsupervised Multitask Learners (2019): https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf - see also https://openai.com/research/better-language-models and https://github.com/openai/gpt-2 (source code) - **Radford et. al.** - 2019

- (GPT-3) Language models are few-shot learners : https://openai.com/research/language-models-are-few-shot-learners and https://arxiv.org/abs/2005.14165 - **Brown et. al.** - May 28, 2020

- Attention Is All You Need: https://arxiv.org/abs/1706.03762 and https://arxiv.org/pdf/1706.03762.pdf - 12 Jun 2017

## Articles and research papers

- Introduction to Weight Quantization : https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c

- GPTQ: ACCURATE POST-TRAINING QUANTIZATION
FOR GENERATIVE PRE-TRAINED TRANSFORMERS : https://arxiv.org/pdf/2210.17323.pdf - 22/03/2023

- Context-faithful Prompting for Large Language Models : https://arxiv.org/abs/2303.11315 - 20/03/2023

- LLaMA: Open and Efficient Foundation Language Models : https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models - 24/02/2023

- Understanding Large Language Models : A Transformative Reading List :  https://sebastianraschka.com/blog/2023/llm-reading-list.html - 07/02/2023

- Openai Research : https://openai.com/research

- Large Language Models are Zero-Shot Reasoners : https://arxiv.org/abs/2205.11916 - 24/02/2022

- (GPT1) Improving Language Understanding
by Generative Pre-Training : https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf - see also https://openai.com/research/language-unsupervised and https://huggingface.co/docs/transformers/model_doc/openai-gpt - 2018

- (GPT-2) Language Models are Unsupervised Multitask Learners (2019): https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf - see also https://openai.com/research/better-language-models and https://github.com/openai/gpt-2 (source code) - 2019

- (GPT-3) Language models are few-shot learners : https://openai.com/research/language-models-are-few-shot-learners and https://arxiv.org/abs/2005.14165 - May 28, 2020

- Fast Transformer Decoding: One Write-Head is All You Need : https://arxiv.org/abs/1911.02150 - 06/11/2019

- How GPT3 Works - Visualizations and Animations - https://jalammar.github.io/how-gpt3-works-visualizations-animations/

- The Illustrated GPT-2 (Visualizing Transformer Language Models) : https://jalammar.github.io/illustrated-gpt2

- Generalized Language Models : https://lilianweng.github.io/posts/2019-01-31-lm - 31/01/2019

- Language Models are Unsupervised Multitask Learners : https://paperswithcode.com/paper/language-models-are-unsupervised-multitask -> https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf - 2019

- A Visual Guide to Using BERT for the First Time : https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time - 2018

- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding : https://arxiv.org/abs/1810.04805 - 11/10/2018

- Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing : https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html - 02/11/2018

- Attention? Attention! : https://lilianweng.github.io/posts/2018-06-24-attention - 24/06/2018

- The Annotated Transformer : http://nlp.seas.harvard.edu/2018/04/03/attention.html - 03/04/2018

- Attention Is All You Need : https://arxiv.org/abs/1706.03762 and https://arxiv.org/pdf/1706.03762.pdf - 12/06/2017

## Resources

### Github and softwares

- bitsandbytes : https://github.com/TimDettmers/bitsandbytes - a lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and quantization functions

- BIG-Bench Hard : https://github.com/suzgunmirac/BIG-Bench-Hard

- Dalai : https://github.com/cocktailpeanut/dalai - Run LLaMA and Alpaca on your computer

- GGML : https://github.com/ggerganov/ggml

- koboldcpp : https://github.com/LostRuins/koboldcpp/wiki

- langchain : https://github.com/langchain-ai/langchain

- LoRA: Low-Rank Adaptation of Large Language Models : https://github.com/microsoft/LoRA

- Llama : https://github.com/facebookresearch/llama

- llama.cpp : https://github.com/ggerganov/llama.cpp

- LMSYS - Fastchat: https://github.com/lm-sys/FastChat -> Vicuna : https://lmsys.org/blog/2023-03-30-vicuna - Cost of training Vicuna-13B is around $300 -> Vicuna Installation Guide : https://github.com/vicuna-tools/vicuna-installation-guide

- Med-Flamingo : https://github.com/snap-stanford/med-flamingo

- Parameter-Efficient Fine-Tuning (PEFT) methods : https://github.com/huggingface/peft - enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters

- QLoRA: Efficient Finetuning of Quantized LLMs : https://github.com/artidoro/qlora

- RedPajama-Data : https://github.com/togethercomputer/RedPajama-Data - An Open Source Recipe to Reproduce LLaMA training dataset

- Stable Diffusion web UI : https://github.com/AUTOMATIC1111/stable-diffusion-webui

- Web LLM : https://github.com/mlc-ai/web-llm

- WizardVicunaLM : https://github.com/melodysdreamj/WizardVicunaLM - Wizard's dataset + ChatGPT's conversation extension + Vicuna's tuning method

- Huggingface : https://huggingface.co

- Modular - mojo : https://www.modular.com/mojo

- OpenAI : https://openai.com

- Midjourney : https://www.midjourney.com

- Stable diffusion : https://stability.ai/stablediffusion




### Learning/Education

- Practical Deep Learning : https://course.fast.ai/Lessons/lesson1.html

- Introduction to Large Lange Models : https://docs.cohere.com/docs/introduction-to-large-language-models

- LLM University! : https://docs.cohere.com/docs/llmu

- LLM Bootcamp - Spring 2023 : https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/

- Machine learning mastery : https://machinelearningmastery.com/start-here/

- The transformer model, explained clearly : https://www.deriveit.org/notes/119

- Geometric Deep Learning : https://geometricdeeplearning.com/blogs/

- Attention is all you need; Attentional Neural Network Models : https://www.youtube.com/watch?v=rBCqOTEfxvg - Oct 2017

- How GPT3 Works - Easily Explained with Animations : https://www.youtube.com/watch?v=MQnJZuBGmSQ - Aug 2020 - Basic overview

- The Narrated Transformer Language Model : https://www.youtube.com/watch?v=-QH8fRhqFHM - Aug 2020 - Details on architecture

- Interfaces for Explaining Transformer Language Models: https://jalammar.github.io/explaining-transformers - 2020

- The little book of deep learning : https://fleuret.org/public/lbdl.pdf


### Discord and related github
- Dalai: https://discord.gg/WWfgrzzkCT and https://github.com/cocktailpeanut/dalai - Run LLaMA and Alpaca on your computer

- FastChat (LMSys) : https://discord.gg/HSWAKCrnFx and https://github.com/lm-sys/FastChat and https://chat.lmsys.org - FastChat is an open platform for training, serving, and evaluating large language model based chatbots

- Text generation web UI: https://discord.gg/jwZCF2dPQN and https://github.com/oobabooga/text-generation-webui

- Tom Jobbins (TheBloke) : https://discord.gg/theblokeai and https://github.com/TheBloke

